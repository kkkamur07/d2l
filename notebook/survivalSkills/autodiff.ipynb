{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef4d6ef",
   "metadata": {},
   "source": [
    "### _Automatic differentiation_ \n",
    "- Builds a computational graph and tracks how the values depend upon each other\n",
    "- Backpropagation : Computational algorithmn applying chain rule in autodifferentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2a56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d028a1f",
   "metadata": {},
   "source": [
    "Differentiation of $y = 2x^Tx$ wrt to $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3588e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "x = torch.arange(0, 5, dtype = torch.float32) # Place to store it and gradient will be of same shape. \n",
    "x.requires_grad_(True)\n",
    "x.grad # Place gradient is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb489dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#* The gradient is stored in the x.grad variable and accumulated there. \n",
    "y = 2 * torch.dot(x,x) # y = 2 * x^T * x\n",
    "\n",
    "# Calculating the gradient\n",
    "y.backward() # This is where the gradient is calculated\n",
    "x.grad\n",
    "\n",
    "# Resetting the gradient\n",
    "x.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "738d0103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0, 5, dtype = torch.float32)\n",
    "x.requires_grad_(True)\n",
    "y = x.sum()\n",
    "y.backward() # Kind of like jacobian\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ba360",
   "metadata": {},
   "source": [
    "In essence, the code calculates the derivative of $y = x0 + x1 + x2 + x3 + x4$ with respect to each $x_i$, which is simply 1 for all i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d2525e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6., 8.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0, 5, dtype = torch.float32)\n",
    "x.requires_grad_(True)\n",
    "y = x*x\n",
    "y.backward(torch.ones_like(x))\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bcbd857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detaching computations : For auxillary intermediate variables\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach() # Detaching from computational graph & stops gradient tracking\n",
    "z = u * x # treats u as a constant\n",
    "z.sum().backward() \n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89915f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maze of control flow logic\n",
    "\n",
    "def f(a) : \n",
    "    b = a * 2\n",
    "    while b.norm() < 1000 :\n",
    "        b = b * 2\n",
    "    if b.sum() > 0 :\n",
    "        c = b\n",
    "    else : \n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(1, requires_grad = True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "a.grad == d/a # Intereseting, didn't understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f312ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 4\n",
    "x = torch.arange(3, requires_grad = True, dtype = torch.float32)\n",
    "y = torch.sin(x)\n",
    "y.sum().backward()\n",
    "x.grad == torch.cos(x) # Gradient of sin(x) is cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4eafcfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     nan,   7.4147,   1.4120, -10.5169, -11.0159])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 5\n",
    "x = torch.arange(5, requires_grad = True, dtype = torch.float32)\n",
    "y = torch.dot(torch.log(x*x), torch.sin(x)) + 1/x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adda841",
   "metadata": {},
   "source": [
    "### 3 Rules \n",
    "- Attach gradients to those with vars wrt to which we desire derivatives\n",
    "- Record the computation of target values\n",
    "- Execute backpropagation function\n",
    "- Access the resulting gradient\n",
    "\n",
    "### Functions\n",
    "1. `x.requires_grad_(True)` : For tracking into the computational graph\n",
    "2. `y.backward()` : $\\frac{dy}{dx}$\n",
    "3. `u = y.detach()` : Detach from the computational graph and treat u as constant\n",
    "4. `x.grad` : Place where the gradients are stored\n",
    "5. `x.grad.zero_()` : Reset the gradient to zero\n",
    "\n",
    "### Tasks \n",
    "- Understand in more depth how backpropagation works\n",
    "  - Using some complicated equations create.\n",
    "- Understand how the computational graph is created\n",
    "- Create your own automatic gradient engine\n",
    "- What is the difference between forward and backward differentiation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
