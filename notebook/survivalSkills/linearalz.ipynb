{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8531e1b8",
   "metadata": {},
   "source": [
    "### Linear Alzebra\n",
    "\n",
    "To build sophisticated models we will be needing tools from `Linear Algebra`\n",
    "\n",
    "$x$ is a scalar : lower case\n",
    "\n",
    "vectors as fixed-length array of scalars. Basically scalars are elements of vectors\n",
    "\n",
    "Response : Loan Default\n",
    "Features : [Income, Length of employment, Previous Defaults] -> Vector of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446748ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0000294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalars\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "z = x + y\n",
    "print(z)  # Output: tensor(3.)\n",
    "\n",
    "# Vectors\n",
    "vec = torch.tensor([1.0, 2.0, 3.0])\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[0, 5],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [3, 8],\n",
      "        [4, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Matrices R \\belong 3*3 -> 3*3 Scalars aranged as 3 rows and 3 columns\n",
    "mat = torch.arange(10).reshape(2,5)\n",
    "print(mat)\n",
    "\n",
    "# Transpose\n",
    "print(mat.T) # Columns <-> Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5c4c5",
   "metadata": {},
   "source": [
    "Symmetric matrices $A = A^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2629726c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((2,2))\n",
    "b = a.T\n",
    "a == b # Symmetric matricess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e36c411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4],\n",
       "        [ 9, 16]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors can have arbitary number of axes\n",
    "# Image as a tensor : Height, width & channel, collection of image as 4th order tensor\n",
    "\n",
    "# Tensor Arithmetic\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = a.clone() # new memory location assigned\n",
    "a, a + b\n",
    "\n",
    "# Hadamard Product\n",
    "a * b # Same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170881be",
   "metadata": {},
   "source": [
    "Elementwise product of tensors is called hadamard product\n",
    "\n",
    "\\begin{split}\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38177de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [7]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduction\n",
    "a.sum() , a.sum(axis = 0) # Reduce the row dimensions\n",
    "a.sum(axis = [0,1]) # Reduce all dimensions\n",
    "b = torch.arange(4, dtype = torch.float32).reshape(2,2)\n",
    "b.mean(axis = 0) # Mean of each column\n",
    "\n",
    "# Non reduction : Useful to keep the dimensions\n",
    "a.sum(axis = 1, keepdims = True) # Keep the dimensions of the columns, useful for broadcasting purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd41763",
   "metadata": {},
   "source": [
    "#### Dot product\n",
    "$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$\n",
    "Product then product of the terms are summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4021b435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]), tensor([1., 1., 1.]), tensor(3.), tensor(3.))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(3, dtype = torch.float32)\n",
    "y = torch.ones(3, dtype = torch.float32)\n",
    "\n",
    "x, y, torch.dot(x, y) , torch.sum(x * y) # Equivalently\n",
    "\n",
    "# Uses \n",
    "# Weighted sum of values as dot product\n",
    "# sum(weights) = 1, dot product express weighted average\n",
    "# after normalizing, dot product represent the cosine angel between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08e858",
   "metadata": {},
   "source": [
    "### Matrix Vector product\n",
    "\n",
    "Row mulitplied by the column of the vectors - nothing special\n",
    "\n",
    "\\begin{split}\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\\end{split}\n",
    "\n",
    "\n",
    "#### Matrix Mulitplication\n",
    "\n",
    "\\begin{split}\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\\end{split}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64eba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([2., 3., 4.]),\n",
       " tensor([11., 38.]),\n",
       " tensor([11., 38.]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype = torch.float32).reshape(2,3)\n",
    "y = torch.tensor([2.0, 3.0, 4.0], dtype = torch.float32)\n",
    "\n",
    "x, y, torch.mv(x,y), x@y # Each row mulitplied by the column vector\n",
    "\n",
    "# @ is the matrix convenience operator - mat vec / mat mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cbd5701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.]]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 3.,  4.,  5.],\n",
       "         [ 9., 14., 19.],\n",
       "         [15., 24., 33.]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype = torch.float32).reshape(2,3)\n",
    "y = torch.arange(6, dtype = torch.float32).reshape(3,2)\n",
    "\n",
    "y,x,y@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58673596",
   "metadata": {},
   "source": [
    "#### Norms - most useful operators\n",
    "- Tells us how big the vector is : Measures the euclidean distance\n",
    "\n",
    "Defination : Norm is a function vector -> scalar\n",
    "- $\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.$\n",
    "- $\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.$ Triange inequality\n",
    "- $\\|\\mathbf{x}\\| > 0 \\textrm{ for all } \\mathbf{x} \\neq 0.$\n",
    "\n",
    "Different norms encode different notions of size.\n",
    "\n",
    "Euclidean Norm : $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.$\n",
    "Manhattan Norm : $\\|x|_1 = \\sum_{i=1}^n |x_i|$\n",
    "\n",
    "$l_p$ Norm : $\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$\n",
    "\n",
    "In matrix norms are complicated because they can be viewed as collection of vectors. \n",
    "- Spectral Norm \n",
    "- Frobenius Norm : $\\|\\mathbf{X}\\|_\\textrm{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$\n",
    "\n",
    "Behaves as if the $l_2$ norm\n",
    "\n",
    "\n",
    "#### Optimization problems\n",
    "Often involve in either maximizing or minimizing of the certain things, often those certain things are distances and distances are represented by norms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76346f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.4944)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype = torch.float32)\n",
    "torch.norm(x), torch.abs(x).sum() # L2 / L1\n",
    "\n",
    "y = torch.arange(12, dtype = torch.float32).reshape(3,4)\n",
    "torch.norm(y) # L1 norm of each row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ba661",
   "metadata": {},
   "source": [
    "1. $(A^T)^T = A$\n",
    "2. $A^T + B^T = (A+B)^T$\n",
    "5. Distance i need to cover is the sum of distances of all streets and avenues\n",
    "10. $(AB)C$ or $A(BC)$ No difference in terms of memory\n",
    "    - (AB)C : 2^10 * 2^14\n",
    "    - A(BC) : 2^10 * 2^14\n",
    "    - Speed yes : A(BC) < (AB)C : Depending upon the size of the intermediate matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e375a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 600])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "a = torch.arange(6, dtype = torch.float32).reshape(2,3)\n",
    "b = a.T\n",
    "c = b.T\n",
    "a==c\n",
    "\n",
    "# Question 2\n",
    "a = torch.arange(6, dtype = torch.float32).reshape(2,3)\n",
    "b = torch.arange(6, dtype = torch.float32).reshape(2,3)\n",
    "(a.T + b.T) == (a + b).T # Sum of transpose is equal to transpose of sum\n",
    "\n",
    "# Question 3 \n",
    "\n",
    "len(torch.zeros((2,3,4))) # 2 Blocks of 3*4, axis = 0\n",
    "torch.numel(torch.zeros((2,3,4))) # 2*3*4 = 24\n",
    "\n",
    "# Question 4\n",
    "a = torch.arange(6, dtype = torch.float32).reshape(2,3)\n",
    "# a / a.sum(axis = 1) # because of dimension reduction we cannot do the operation\n",
    "\n",
    "# Question 8\n",
    "a = torch.arange(24, dtype = torch.float32).reshape(2,3,4)\n",
    "a.sum(axis = 0), a.sum(axis = 1), a.sum(axis = 2) # Basically sum across that axis - axis gone.\n",
    "\n",
    "# Question 12\n",
    "a = torch.ones((100,200))\n",
    "b = torch.ones((100,200))\n",
    "c = torch.ones((100,200))\n",
    "\n",
    "torch.cat([a,b,c]).shape # 300 * 200\n",
    "torch.cat([a,b,c], axis = 1).shape # 100 * 600, stacked along the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d154d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
