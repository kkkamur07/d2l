{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7872eeb5",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d6823",
   "metadata": {},
   "source": [
    "Transformer is a family of neural network architectures that came to computer vision from NLP. Since transformers don't assume that their input has any specific structure, they can learn more general dependencies in data than convolutional neural network architectures. That's why we all like Vision transformers. At the same time, vision transformes are known to be \"data hungry\" and their training is quite tricky.\n",
    "\n",
    "In this homework we will go through main components of vision transformers and their training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d5684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb93c2",
   "metadata": {},
   "source": [
    "## How to code your transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8ab73",
   "metadata": {},
   "source": [
    "As it was said, vision transformer came from NLP area where typical neural network input is ordered sequence of tokens which are words or word parts. So vision transformer main blocks are:\n",
    "1. Tokenizer - module that takes images and returns a set of tokens\n",
    "2. Transformer encoder - the main block of neural network that contains multihead attention, normalization and MLP on tokens.\n",
    "3. Positional embeddings - a way how to provide information about token orders\n",
    "4. Classification token - special token whose features is expected to be used for the final class prediction\n",
    "5. Classification head - MLP that predicts the final class from classificaiton token features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2f4c",
   "metadata": {},
   "source": [
    "### Tokenizer (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c007",
   "metadata": {},
   "source": [
    "Tokenizer should take an image, split it on non-overlapping patches, flatten the patches and apply Linear layer to these vectors. There are many ways how one can implement this, we will do it using Conv2D with stride being equal to kernel_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50abdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, input_height, input_width, output_height, output_width,\n",
    "                 n_input_channels,\n",
    "                 embedding_dim):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        assert input_height % output_height == 0, f\"{input_height} should be devided by {output_height}\"\n",
    "        assert input_width % output_width == 0, f\"{input_width} should be devided by {output_width}\"\n",
    "        \n",
    "        kernel_size = input_height // output_height\n",
    "        assert kernel_size == input_width // output_width\n",
    "\n",
    "        \n",
    "        self.conv = nn.Conv2d(n_input_channels, embedding_dim, stride=kernel_size, kernel_size=kernel_size)\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv(x)).transpose(-2, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdf6e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_height=32, input_width=32, output_height=16, output_width=16, n_input_channels=1,\n",
    "                      embedding_dim=64)\n",
    "dummy_batch = torch.zeros((1, 1, 32, 32))\n",
    "tokenizer_result = tokenizer.forward(dummy_batch)\n",
    "assert tokenizer_result.shape[1] == 16*16, tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7e5e",
   "metadata": {},
   "source": [
    "### Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d258b",
   "metadata": {},
   "source": [
    "Transformer encoder consists of 2 blocks: Multi-Head Attention and MLP, each of each is prepended by layer norm. Let's walk through the separate modules for beggining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aee083",
   "metadata": {},
   "source": [
    "#### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b99e7f",
   "metadata": {},
   "source": [
    "<img src=\"https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eacf00",
   "metadata": {},
   "source": [
    "Attention implements a simple formula: $\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$.\n",
    "\n",
    "Multi-head attention is about splitting Q, K, V on several subvectors, appling Attention on each subvector independendly and concating the result.\n",
    "\n",
    "You can find Multi-Head Attention being implemented in pytorch as `torch.nn.MultiheadAttention`. Check the documentation and pay attention on `dropout` and `batch_first` parameters.\n",
    "\n",
    "[[paper]](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5327af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "torch.nn.MultiheadAttention(\n",
      "    embed_dim,\n",
      "    num_heads,\n",
      "    dropout=\u001b[32m0.0\u001b[39m,\n",
      "    bias=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    add_bias_kv=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    add_zero_attn=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    kdim=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    vdim=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    batch_first=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    device=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    dtype=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m MultiheadAttention(Module):\n",
      "    \u001b[33mr\"\"\"Allows the model to jointly attend to information\u001b[39m\n",
      "\u001b[33m    from different representation subspaces as described in the paper:\u001b[39m\n",
      "\u001b[33m    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\u001b[39m\n",
      "\n",
      "\u001b[33m    Multi-Head Attention is defined as:\u001b[39m\n",
      "\n",
      "\u001b[33m    .. math::\u001b[39m\n",
      "\u001b[33m        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\u001b[39m\n",
      "\n",
      "\u001b[33m    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    ``forward()`` will use the optimized implementation described in\u001b[39m\n",
      "\u001b[33m    `FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`_ if all of the following\u001b[39m\n",
      "\u001b[33m    conditions are met:\u001b[39m\n",
      "\n",
      "\u001b[33m    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor. This\u001b[39m\n",
      "\u001b[33m      restriction will be loosened in the future.)\u001b[39m\n",
      "\u001b[33m    - inputs are batched (3D) with ``batch_first==True``\u001b[39m\n",
      "\u001b[33m    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\u001b[39m\n",
      "\u001b[33m    - training is disabled (using ``.eval()``)\u001b[39m\n",
      "\u001b[33m    - ``add_bias_kv`` is ``False``\u001b[39m\n",
      "\u001b[33m    - ``add_zero_attn`` is ``False``\u001b[39m\n",
      "\u001b[33m    - ``batch_first`` is ``True`` and the input is batched\u001b[39m\n",
      "\u001b[33m    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\u001b[39m\n",
      "\u001b[33m    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\u001b[39m\n",
      "\u001b[33m      nor ``attn_mask`` is passed\u001b[39m\n",
      "\u001b[33m    - autocast is disabled\u001b[39m\n",
      "\n",
      "\u001b[33m    If the optimized implementation is in use, a\u001b[39m\n",
      "\u001b[33m    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\u001b[39m\n",
      "\u001b[33m    ``query``/``key``/``value`` to represent padding more efficiently than using a\u001b[39m\n",
      "\u001b[33m    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\u001b[39m\n",
      "\u001b[33m    will be returned, and an additional speedup proportional to the fraction of the input\u001b[39m\n",
      "\u001b[33m    that is padding can be expected.\u001b[39m\n",
      "\n",
      "\u001b[33m    Args:\u001b[39m\n",
      "\u001b[33m        embed_dim: Total dimension of the model.\u001b[39m\n",
      "\u001b[33m        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\u001b[39m\n",
      "\u001b[33m            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\u001b[39m\n",
      "\u001b[33m        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\u001b[39m\n",
      "\u001b[33m        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\u001b[39m\n",
      "\u001b[33m        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\u001b[39m\n",
      "\u001b[33m        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\u001b[39m\n",
      "\u001b[33m            Default: ``False``.\u001b[39m\n",
      "\u001b[33m        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\u001b[39m\n",
      "\u001b[33m        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\u001b[39m\n",
      "\u001b[33m        batch_first: If ``True``, then the input and output tensors are provided\u001b[39m\n",
      "\u001b[33m            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\u001b[39m\n",
      "\n",
      "\u001b[33m    Examples::\u001b[39m\n",
      "\n",
      "\u001b[33m        >>> # xdoctest: +SKIP\u001b[39m\n",
      "\u001b[33m        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\u001b[39m\n",
      "\u001b[33m        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\u001b[39m\n",
      "\n",
      "\u001b[33m    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\u001b[39m\n",
      "\u001b[33m         https://arxiv.org/abs/2205.14135\u001b[39m\n",
      "\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "    __constants__ = [\u001b[33m'batch_first'\u001b[39m]\n",
      "    bias_k: Optional[torch.Tensor]\n",
      "    bias_v: Optional[torch.Tensor]\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(self, embed_dim, num_heads, dropout=\u001b[32m0.\u001b[39m, bias=\u001b[38;5;28;01mTrue\u001b[39;00m, add_bias_kv=\u001b[38;5;28;01mFalse\u001b[39;00m, add_zero_attn=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "                 kdim=\u001b[38;5;28;01mNone\u001b[39;00m, vdim=\u001b[38;5;28;01mNone\u001b[39;00m, batch_first=\u001b[38;5;28;01mFalse\u001b[39;00m, device=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        factory_kwargs = {\u001b[33m'device'\u001b[39m: device, \u001b[33m'dtype'\u001b[39m: dtype}\n",
      "        super().__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "        self.kdim = kdim \u001b[38;5;28;01mif\u001b[39;00m kdim \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m embed_dim\n",
      "        self.vdim = vdim \u001b[38;5;28;01mif\u001b[39;00m vdim \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m embed_dim\n",
      "        self._qkv_same_embed_dim = self.kdim == embed_dim \u001b[38;5;28;01mand\u001b[39;00m self.vdim == embed_dim\n",
      "\n",
      "        self.num_heads = num_heads\n",
      "        self.dropout = dropout\n",
      "        self.batch_first = batch_first\n",
      "        self.head_dim = embed_dim // num_heads\n",
      "        \u001b[38;5;28;01massert\u001b[39;00m self.head_dim * num_heads == self.embed_dim, \u001b[33m\"embed_dim must be divisible by num_heads\"\u001b[39m\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._qkv_same_embed_dim:\n",
      "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
      "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
      "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
      "            self.register_parameter(\u001b[33m'in_proj_weight'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            self.in_proj_weight = Parameter(torch.empty((\u001b[32m3\u001b[39m * embed_dim, embed_dim), **factory_kwargs))\n",
      "            self.register_parameter(\u001b[33m'q_proj_weight'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "            self.register_parameter(\u001b[33m'k_proj_weight'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "            self.register_parameter(\u001b[33m'v_proj_weight'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m bias:\n",
      "            self.in_proj_bias = Parameter(torch.empty(\u001b[32m3\u001b[39m * embed_dim, **factory_kwargs))\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            self.register_parameter(\u001b[33m'in_proj_bias'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m add_bias_kv:\n",
      "            self.bias_k = Parameter(torch.empty((\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, embed_dim), **factory_kwargs))\n",
      "            self.bias_v = Parameter(torch.empty((\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, embed_dim), **factory_kwargs))\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            self.bias_k = self.bias_v = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "        self.add_zero_attn = add_zero_attn\n",
      "\n",
      "        self._reset_parameters()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _reset_parameters(self):\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._qkv_same_embed_dim:\n",
      "            xavier_uniform_(self.in_proj_weight)\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            xavier_uniform_(self.q_proj_weight)\n",
      "            xavier_uniform_(self.k_proj_weight)\n",
      "            xavier_uniform_(self.v_proj_weight)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.in_proj_bias \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            constant_(self.in_proj_bias, \u001b[32m0.\u001b[39m)\n",
      "            constant_(self.out_proj.bias, \u001b[32m0.\u001b[39m)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.bias_k \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            xavier_normal_(self.bias_k)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.bias_v \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            xavier_normal_(self.bias_v)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __setstate__(self, state):\n",
      "        \u001b[38;5;66;03m# Support loading old MultiheadAttention checkpoints generated by v1.1.0\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'_qkv_same_embed_dim'\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m state:\n",
      "            state[\u001b[33m'_qkv_same_embed_dim'\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "        super().__setstate__(state)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m forward(\n",
      "            self,\n",
      "            query: Tensor,\n",
      "            key: Tensor,\n",
      "            value: Tensor,\n",
      "            key_padding_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "            need_weights: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "            attn_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "            average_attn_weights: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "            is_causal : bool = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Tuple[Tensor, Optional[Tensor]]:\n",
      "        \u001b[33mr\"\"\"\u001b[39m\n",
      "\u001b[33m    Args:\u001b[39m\n",
      "\u001b[33m        query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\u001b[39m\n",
      "\u001b[33m            or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\u001b[39m\n",
      "\u001b[33m            :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\u001b[39m\n",
      "\u001b[33m            Queries are compared against key-value pairs to produce the output.\u001b[39m\n",
      "\u001b[33m            See \"Attention Is All You Need\" for more details.\u001b[39m\n",
      "\u001b[33m        key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\u001b[39m\n",
      "\u001b[33m            or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\u001b[39m\n",
      "\u001b[33m            :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\u001b[39m\n",
      "\u001b[33m            See \"Attention Is All You Need\" for more details.\u001b[39m\n",
      "\u001b[33m        value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\u001b[39m\n",
      "\u001b[33m            ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\u001b[39m\n",
      "\u001b[33m            sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\u001b[39m\n",
      "\u001b[33m            See \"Attention Is All You Need\" for more details.\u001b[39m\n",
      "\u001b[33m        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\u001b[39m\n",
      "\u001b[33m            to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\u001b[39m\n",
      "\u001b[33m            Binary and float masks are supported.\u001b[39m\n",
      "\u001b[33m            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\u001b[39m\n",
      "\u001b[33m            the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\u001b[39m\n",
      "\u001b[33m        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\u001b[39m\n",
      "\u001b[33m            Default: ``True``.\u001b[39m\n",
      "\u001b[33m        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\u001b[39m\n",
      "\u001b[33m            :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\u001b[39m\n",
      "\u001b[33m            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\u001b[39m\n",
      "\u001b[33m            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\u001b[39m\n",
      "\u001b[33m            Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\u001b[39m\n",
      "\u001b[33m            corresponding position is not allowed to attend. For a float mask, the mask values will be added to\u001b[39m\n",
      "\u001b[33m            the attention weight.\u001b[39m\n",
      "\u001b[33m            If both attn_mask and key_padding_mask are supplied, their types should match.\u001b[39m\n",
      "\u001b[33m        is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\u001b[39m\n",
      "\u001b[33m            Default: ``False``.\u001b[39m\n",
      "\u001b[33m        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\u001b[39m\n",
      "\u001b[33m            heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\u001b[39m\n",
      "\u001b[33m            effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\u001b[39m\n",
      "\n",
      "\u001b[33m    Outputs:\u001b[39m\n",
      "\u001b[33m        - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\u001b[39m\n",
      "\u001b[33m          :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\u001b[39m\n",
      "\u001b[33m          where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\u001b[39m\n",
      "\u001b[33m          embedding dimension ``embed_dim``.\u001b[39m\n",
      "\u001b[33m        - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\u001b[39m\n",
      "\u001b[33m          returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\u001b[39m\n",
      "\u001b[33m          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\u001b[39m\n",
      "\u001b[33m          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\u001b[39m\n",
      "\u001b[33m          head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. note::\u001b[39m\n",
      "\u001b[33m            `batch_first` argument is ignored for unbatched inputs.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m is_causal:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m AssertionError(\u001b[33m\"Only allow causal mask or attn_mask\"\u001b[39m)\n",
      "\n",
      "        is_batched = query.dim() == \u001b[32m3\u001b[39m\n",
      "\n",
      "        key_padding_mask = F._canonical_mask(\n",
      "            mask=key_padding_mask,\n",
      "            mask_name=\u001b[33m\"key_padding_mask\"\u001b[39m,\n",
      "            other_type=F._none_or_dtype(attn_mask),\n",
      "            other_name=\u001b[33m\"attn_mask\"\u001b[39m,\n",
      "            target_type=query.dtype\n",
      "        )\n",
      "\n",
      "        why_not_fast_path = \u001b[33m''\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_batched:\n",
      "            why_not_fast_path = \u001b[33mf\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m query \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m key \u001b[38;5;28;01mor\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m value:\n",
      "            \u001b[38;5;66;03m# When lifting this restriction, don't forget to either\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# enforce that the dtypes all match or test cases where\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# they don't!\u001b[39;00m\n",
      "            why_not_fast_path = \u001b[33m\"non-self attention was used (query, key, and value are not the same Tensor)\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.in_proj_bias \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m query.dtype != self.in_proj_bias.dtype:\n",
      "            why_not_fast_path = \u001b[33mf\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.in_proj_weight \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m query.dtype != self.in_proj_weight.dtype:\n",
      "            \u001b[38;5;66;03m# this case will fail anyway, but at least they'll get a useful error message.\u001b[39;00m\n",
      "            why_not_fast_path = \u001b[33mf\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.training:\n",
      "            why_not_fast_path = \u001b[33m\"training is enabled\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.batch_first:\n",
      "            why_not_fast_path = \u001b[33m\"batch_first was not True\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.bias_k \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            why_not_fast_path = \u001b[33m\"self.bias_k was not None\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.bias_v \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            why_not_fast_path = \u001b[33m\"self.bias_v was not None\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.add_zero_attn:\n",
      "            why_not_fast_path = \u001b[33m\"add_zero_attn was enabled\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._qkv_same_embed_dim:\n",
      "            why_not_fast_path = \u001b[33m\"_qkv_same_embed_dim was not True\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m query.is_nested \u001b[38;5;28;01mand\u001b[39;00m (key_padding_mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m attn_mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "            why_not_fast_path = \u001b[33m\"supplying both src_key_padding_mask and src_mask at the same time \\\u001b[39m\n",
      "\u001b[33m                                 is not supported with NestedTensor input\"\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m torch.is_autocast_enabled():\n",
      "            why_not_fast_path = \u001b[33m\"autocast is enabled\"\u001b[39m\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m why_not_fast_path:\n",
      "            tensor_args = (\n",
      "                query,\n",
      "                key,\n",
      "                value,\n",
      "                self.in_proj_weight,\n",
      "                self.in_proj_bias,\n",
      "                self.out_proj.weight,\n",
      "                self.out_proj.bias,\n",
      "            )\n",
      "            \u001b[38;5;66;03m# We have to use list comprehensions below because TorchScript does not support\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# generator expressions.\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m torch.overrides.has_torch_function(tensor_args):\n",
      "                why_not_fast_path = \u001b[33m\"some Tensor argument has_torch_function\"\u001b[39m\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m all([(x \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m x.is_cuda \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m'cpu'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m str(x.device)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m tensor_args]):\n",
      "                why_not_fast_path = \u001b[33m\"some Tensor argument is neither CUDA nor CPU\"\u001b[39m\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m torch.is_grad_enabled() \u001b[38;5;28;01mand\u001b[39;00m any([x \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m x.requires_grad \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m tensor_args]):\n",
      "                why_not_fast_path = (\u001b[33m\"grad is enabled and at least one of query or the \"\u001b[39m\n",
      "                                     \u001b[33m\"input/output projection weights or biases requires_grad\"\u001b[39m)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m why_not_fast_path:\n",
      "                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n",
      "\n",
      "                \u001b[38;5;28;01mreturn\u001b[39;00m torch._native_multi_head_attention(\n",
      "                    query,\n",
      "                    key,\n",
      "                    value,\n",
      "                    self.embed_dim,\n",
      "                    self.num_heads,\n",
      "                    self.in_proj_weight,\n",
      "                    self.in_proj_bias,\n",
      "                    self.out_proj.weight,\n",
      "                    self.out_proj.bias,\n",
      "                    merged_mask,\n",
      "                    need_weights,\n",
      "                    average_attn_weights,\n",
      "                    mask_type)\n",
      "\n",
      "        any_nested = query.is_nested \u001b[38;5;28;01mor\u001b[39;00m key.is_nested \u001b[38;5;28;01mor\u001b[39;00m value.is_nested\n",
      "        \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m any_nested, (\u001b[33m\"MultiheadAttention does not support NestedTensor outside of its fast path. \"\u001b[39m +\n",
      "                                \u001b[33mf\"The fast path was not hit because {why_not_fast_path}\"\u001b[39m)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.batch_first \u001b[38;5;28;01mand\u001b[39;00m is_batched:\n",
      "            \u001b[38;5;66;03m# make sure that the transpose op does not affect the \"is\" property\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m value:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;28;01mis\u001b[39;00m key:\n",
      "                    query = key = value = query.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m)\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                    query, key = [x.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m (query, key)]\n",
      "                    value = key\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                query, key, value = [x.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m (query, key, value)]\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._qkv_same_embed_dim:\n",
      "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                query, key, value, self.embed_dim, self.num_heads,\n",
      "                self.in_proj_weight, self.in_proj_bias,\n",
      "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
      "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
      "                training=self.training,\n",
      "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
      "                attn_mask=attn_mask,\n",
      "                use_separate_proj_weight=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
      "                v_proj_weight=self.v_proj_weight,\n",
      "                average_attn_weights=average_attn_weights,\n",
      "                is_causal=is_causal)\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                query, key, value, self.embed_dim, self.num_heads,\n",
      "                self.in_proj_weight, self.in_proj_bias,\n",
      "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
      "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
      "                training=self.training,\n",
      "                key_padding_mask=key_padding_mask,\n",
      "                need_weights=need_weights,\n",
      "                attn_mask=attn_mask,\n",
      "                average_attn_weights=average_attn_weights,\n",
      "                is_causal=is_causal)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.batch_first \u001b[38;5;28;01mand\u001b[39;00m is_batched:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_output_weights\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m merge_masks(self, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor],\n",
      "                    query: Tensor) -> Tuple[Optional[Tensor], Optional[int]]:\n",
      "        \u001b[33mr\"\"\"\u001b[39m\n",
      "\u001b[33m        Determine mask type and combine masks if necessary. If only one mask is provided, that mask\u001b[39m\n",
      "\u001b[33m        and the corresponding mask type will be returned. If both masks are provided, they will be both\u001b[39m\n",
      "\u001b[33m        expanded to shape ``(batch_size, num_heads, seq_len, seq_len)``, combined with logical ``or``\u001b[39m\n",
      "\u001b[33m        and mask type 2 will be returned\u001b[39m\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            attn_mask: attention mask of shape ``(seq_len, seq_len)``, mask type 0\u001b[39m\n",
      "\u001b[33m            key_padding_mask: padding mask of shape ``(batch_size, seq_len)``, mask type 1\u001b[39m\n",
      "\u001b[33m            query: query embeddings of shape ``(batch_size, seq_len, embed_dim)``\u001b[39m\n",
      "\u001b[33m        Returns:\u001b[39m\n",
      "\u001b[33m            merged_mask: merged mask\u001b[39m\n",
      "\u001b[33m            mask_type: merged mask type (0, 1, or 2)\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        mask_type: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        merged_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "        attn_mask = F._canonical_mask(\n",
      "            mask=attn_mask,\n",
      "            mask_name=\u001b[33m\"attn_mask\"\u001b[39m,\n",
      "            other_type=F._none_or_dtype(key_padding_mask),\n",
      "            other_name=\u001b[33m\"key_padding_mask\"\u001b[39m,\n",
      "            target_type=query.dtype,\n",
      "            check_other=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            mask_type = \u001b[32m0\u001b[39m\n",
      "            merged_mask = attn_mask\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            mask_type = \u001b[32m1\u001b[39m\n",
      "            merged_mask = key_padding_mask\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m (attn_mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mand\u001b[39;00m (key_padding_mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "            \u001b[38;5;66;03m# In this branch query can't be a nested tensor, so it has a shape\u001b[39;00m\n",
      "            batch_size, seq_len, _ = query.shape\n",
      "            mask_type = \u001b[32m2\u001b[39m\n",
      "            key_padding_mask_expanded = key_padding_mask.view(batch_size, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, seq_len) \\\n",
      "                                                        .expand(-\u001b[32m1\u001b[39m, self.num_heads, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n",
      "            attn_mask_expanded = attn_mask.view(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, seq_len, seq_len).expand(batch_size, self.num_heads, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n",
      "            merged_mask = attn_mask_expanded + key_padding_mask_expanded\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m merged_mask, mask_type\n",
      "\u001b[31mFile:\u001b[39m           ~/Desktop/Programming/python/d2l-learnings/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     MultiheadAttention"
     ]
    }
   ],
   "source": [
    "torch.nn.MultiheadAttention??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b57604",
   "metadata": {},
   "source": [
    "#### MLP for Transformer Encoder (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd93b81",
   "metadata": {},
   "source": [
    "MLP for transformer encoder is just a simple two-layer perceptron with RELU as non-linearity. It also uses Dropout after each Linear layer in order to reduce overfitting. Important thing is that size of hidden state on MLP is usually several times bigger than size of MLP input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "845ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(embedding_dim, mlp_size, dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        # YOUR CODE: Linear + RELU + Dropout + Linear + Dropout\n",
    "        nn.Linear(embedding_dim, mlp_size), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(mlp_size, embedding_dim),\n",
    "        nn.Dropout(dropout_rate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b94014c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = create_mlp(128, 128 * 2, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07328ab",
   "metadata": {},
   "source": [
    "#### Layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970438d",
   "metadata": {},
   "source": [
    "While Batch Normalization is a default normalization layer for convolutional neural networks, in transformers Layer Normalization is used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766c262",
   "metadata": {},
   "source": [
    "Layer norm is implemented in pytorch as `torch.nn.LayerNorm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90723060",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LayerNorm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fdab4",
   "metadata": {},
   "source": [
    "#### TransformerEncoder: putting it all together (2p)\n",
    "\n",
    "Now we are ready to define Transformer Encoder.\n",
    "<img src=\"./transformer_encoder.png\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ddf2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.attention_pre_norm = nn.LayerNorm(embedding_dim) # Changed to LayerNorm\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True) # Added batch_first=True\n",
    "        self.attention_output_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_pre_norm = nn.LayerNorm(embedding_dim) # Added LayerNorm\n",
    "        self.mlp = create_mlp(embedding_dim=embedding_dim, mlp_size=mlp_size, dropout_rate=dropout) #Added dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first block\n",
    "        y = self.attention_pre_norm(x)\n",
    "        attention = self.attention(y, y, y)[0]\n",
    "        attention = self.attention_output_dropout(attention)\n",
    "        x = x + attention   # Residual connection\n",
    "            \n",
    "        # second block\n",
    "        y = self.mlp_pre_norm(x)\n",
    "        y = self.mlp(y)\n",
    "        x = x + y   # Residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d4acf",
   "metadata": {},
   "source": [
    "Let's check that it actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "109eeb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87c8d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "e = TransformerEncoder(embedding_dim=64, num_heads=2, mlp_size=128)\n",
    "encoder_result = e(tokenizer_result)\n",
    "print (encoder_result.shape)\n",
    "assert encoder_result.shape == tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16e020",
   "metadata": {},
   "source": [
    "### Positional embeddings (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec405abd",
   "metadata": {},
   "source": [
    "Positional embeddings is a way to give transformer information about token orders. You can either learn good embeddings by SGD or use some scheme for embeddings generation. The most popular scheme is sinusoidal embeddings:\n",
    "\n",
    "$$\\text{emb}(p, 2i) = \\sin(\\frac{p}{10000^{2i/d}})$$\n",
    "$$\\text{emb}(p, 2i + 1) = \\cos(\\frac{p}{10000^{2i/d}})$$\n",
    "where p, 2i, 2i+1 - indices of embedding element, d - embedding dimension\n",
    "\n",
    "Tranditional way of using embeddings in pytorch is by `torch.nn.Embedding`. But in our case its simplier to use more low-level thing `torch.nn.Parameter`. Here is how one can define learnable embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a607f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 256\n",
    "embedding_dim = 64\n",
    "\n",
    "# YOUR CODE\n",
    "emb =  torch.nn.Parameter(torch.randn((n_tokens, embedding_dim)))\n",
    "\n",
    "_ = torch.nn.init.trunc_normal_(emb, std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1553d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2000, grad_fn=<StdBackward0>) torch.Size([256, 64])\n"
     ]
    }
   ],
   "source": [
    "print(emb.std(), emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9879388",
   "metadata": {},
   "source": [
    "### Class token and classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53167f24",
   "metadata": {},
   "source": [
    "Vanilla Vision Transformer uses a rather unusual way how to get the embedding of the whole image for the final prediction. It adds one more token, named as class-token, with its own positional embedding and takes its features as the final embedding of image. Alternative approach that comes from CNN is to use global average pooling for image embeddings obtaining. While being more simple to implement, global average pooling introduces a shortcut how different patches can communicates between each other (in vanilla ViT all the inter-patch relations can be learned only through attention blocks).\n",
    "\n",
    "However in modern papers you can meet the both approaces equally likely.\n",
    "\n",
    "Adding class token in pytorch is simple thing. You can either add one more embedding to `nn.Parameter` for positional encoders or create one more `nn.Parameter` module for class token only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "356f572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64]) torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "class_emb = torch.nn.Parameter(torch.empty((1, embedding_dim)), requires_grad=True) # One more token\n",
    "torch.nn.init.trunc_normal_(class_emb, std=0.2)\n",
    "\n",
    "print(class_emb[0].shape, class_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93121d04",
   "metadata": {},
   "source": [
    "### Vision Transformer: putting it all together (3p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ccaab8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_height, input_width,\n",
    "                 n_tokens,\n",
    "                 n_input_channels,\n",
    "                 embedding_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 num_classes=1000,\n",
    "                 mlp_ratio=4.0,\n",
    "                 dropout=0.1,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Tokenizer (patch + embed)\n",
    "        self.tokenizer = Tokenizer(\n",
    "            input_height=input_height, input_width=input_width,\n",
    "            output_height=int(input_height ** 0.5),   # assumes square grid → √N_patches\n",
    "            output_width =int(input_width  ** 0.5),\n",
    "            n_input_channels=n_input_channels,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        \n",
    "        # 2. Class token & 3. Positional embedding\n",
    "        self.cls_token      = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "        self.pos_embedding  = nn.Parameter(torch.zeros(1, n_tokens + 1, embedding_dim))\n",
    "        self.dropout        = nn.Dropout(dropout)\n",
    "        \n",
    "        # 4. Transformer encoder blocks\n",
    "        mlp_size   = int(embedding_dim * mlp_ratio)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoder(embedding_dim, num_heads, mlp_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 5. Final LayerNorm\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # 6. Classification head\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs : (B, C, H, W)\n",
    "        returns logits : (B, num_classes)\n",
    "        \"\"\"\n",
    "        B = imgs.size(0)\n",
    "        x = self.tokenizer(imgs)  \n",
    "        \n",
    "        # 2. position embeddings\n",
    "        x = x + self.pos_embedding[:, : x.size(1), :]\n",
    "\n",
    "        # 3. adding class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1) \n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        \n",
    "        # dropout!\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 4. transformer encoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # 5. final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 6. final prediction from class-token embeddings\n",
    "        x = x[:, 0] \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9adde091",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 16\n",
    "input_width = 16\n",
    "n_input_channels = 1\n",
    "vit = VisionTransformer(input_height, input_width,\n",
    "                 n_tokens=4,\n",
    "                 n_input_channels=n_input_channels,\n",
    "                 embedding_dim=32,\n",
    "                 num_layers=2,\n",
    "                 num_heads=2,\n",
    "                 num_classes=10,\n",
    "                 mlp_ratio=2.0,\n",
    "                 dropout=0.1,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f69ac628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m fake_batch = torch.rand((\u001b[32m1\u001b[39m, n_input_channels, input_height, input_width))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(fake_batch.shape)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.shape)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Programming/python/d2l-learnings/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, imgs)\u001b[39m\n\u001b[32m     48\u001b[39m x = \u001b[38;5;28mself\u001b[39m.tokenizer(imgs)  \n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# 2. position embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 3. adding class token\u001b[39;00m\n\u001b[32m     54\u001b[39m cls_tokens = \u001b[38;5;28mself\u001b[39m.cls_token.expand(B, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m) \n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (16) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "fake_batch = torch.rand((1, n_input_channels, input_height, input_width))\n",
    "print(fake_batch.shape)\n",
    "\n",
    "result = vit(fake_batch)\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
