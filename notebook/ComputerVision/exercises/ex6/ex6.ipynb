{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d160129c",
   "metadata": {},
   "source": [
    "#### Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ad5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ec5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module) : \n",
    "    def __init__(self) : \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, query, key, values) : \n",
    "        \"\"\"\n",
    "        query => [batch, num_queries, embeddings]\n",
    "        key ==> [batch, num_objects(vocab), embeddings]\n",
    "        value ==> [batch, num_objects(vocab), value_embeddings]\n",
    "        \n",
    "        \"\"\"\n",
    "        dot_product = query @ key.transpose(-1, -2)\n",
    "        scaled_dot = dot_product / torch.sqrt(torch.tensor(key.shape[-1], dtype=torch.float32))\n",
    "        weights = torch.softmax(scaled_dot, dim = 1) # num_queries dimension\n",
    "        self.attention_map = weights.detach()\n",
    "        result = weights @ values\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30811558",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(2, 3, 5)\n",
    "k = torch.randn(2, 4, 5)\n",
    "v = torch.randn(2, 4, 7)\n",
    "\n",
    "\n",
    "results = Attention()(q,k,v)\n",
    "\n",
    "assert results.shape == (2,3,7) # (batch_size, num_queries, output_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e693eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    def __init__(self, n_tokens=n_tokens, emb_size=128, lstm_units=256, cnn_channels=512):\n",
    "        \"\"\" A recurrent 'head' network for image captioning. Read scheme below. \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        # a layer that converts conv features to initial LSTM states\n",
    "        self.cnn_to_h0 = nn.Linear(cnn_channels, lstm_units)\n",
    "        self.cnn_to_c0 = nn.Linear(cnn_channels, lstm_units)\n",
    "\n",
    "        # create embedding for input words. Use the parameters (e.g. emb_size).\n",
    "        self.emb = nn.Embedding(n_tokens, emb_size)\n",
    "\n",
    "        # attention: create attention over image spatial positions\n",
    "        # The query is previous lstm hidden state, the keys are transformed cnn features,\n",
    "        # the values are cnn features\n",
    "        self.attention = Attention(ScaledDotProductScore())\n",
    "\n",
    "        # attention: create transform from cnn features to the keys\n",
    "        # Hint: one linear layer should work\n",
    "        # Hint: the dimensionality of keys should be lstm_units as lstm\n",
    "        #       hidden state is the attention query\n",
    "        self.cnn_to_attn_key = nn.Linear(cnn_channels, lstm_units)\n",
    "\n",
    "        # lstm: create a recurrent core of your network. Use LSTMCell\n",
    "        self.lstm = nn.LSTMCell(emb_size + cnn_channels, lstm_units)\n",
    "\n",
    "        # create logits: MLP that takes attention response, lstm hidden state\n",
    "        # and the previous word embedding as an input and computes one number per token\n",
    "        # Hint: I used an architecture with one hidden layer, but you may try deeper ones\n",
    "        self.logits_mlp = nn.Sequential(\n",
    "            nn.Linear(lstm_units + cnn_channels + emb_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_tokens)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features, captions_ix):\n",
    "        \"\"\"\n",
    "        Apply the network in training mode.\n",
    "        :param image_features: torch tensor containing VGG features for each position.\n",
    "                               shape: [batch, cnn_channels, width * height]\n",
    "        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i].\n",
    "            padded with pad_ix\n",
    "        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n",
    "        \"\"\"\n",
    "        batch_size, cnn_channels, spatial_size = image_features.shape\n",
    "        caption_length = captions_ix.shape[1]\n",
    "\n",
    "        # Initialize LSTM states from averaged image features\n",
    "        initial_cell = self.cnn_to_c0(image_features.mean(2))  # [batch, lstm_units]\n",
    "        initial_hid = self.cnn_to_h0(image_features.mean(2))   # [batch, lstm_units]\n",
    "\n",
    "        # Transpose to [batch, spatial_size, cnn_channels] for attention\n",
    "        image_features = image_features.transpose(1, 2)\n",
    "\n",
    "        # compute embeddings for captions_ix\n",
    "        captions_emb = self.emb(captions_ix)  # [batch, caption_length, emb_size]\n",
    "\n",
    "        # apply recurrent layer to captions_emb.\n",
    "        # 1. initialize lstm state with initial_* from above\n",
    "        h_t, c_t = initial_hid, initial_cell\n",
    "\n",
    "        # Lists to store outputs\n",
    "        recurrent_outputs = []\n",
    "        attention_maps = []\n",
    "\n",
    "        # 2. In the recurrent loop over tokens:\n",
    "        for t in range(caption_length):\n",
    "            # 2.1. transform image vectors to the keys for attention\n",
    "            image_keys = self.cnn_to_attn_key(image_features)  # [batch, spatial_size, lstm_units]\n",
    "\n",
    "            # 2.2. use previous lstm state as an attention query and image vectors as values\n",
    "            query = h_t.unsqueeze(1)  # [batch, 1, lstm_units] - add query dimension\n",
    "\n",
    "            # 2.3. apply attention to obtain context vector\n",
    "            context = self.attention(query, image_keys, image_features)  # [batch, 1, cnn_channels]\n",
    "            context = context.squeeze(1)  # [batch, cnn_channels] - remove query dimension\n",
    "\n",
    "            # 2.4. store attention map\n",
    "            attention_maps.append(self.attention.attention_map.squeeze(1))  # [batch, spatial_size]\n",
    "\n",
    "            # Get current word embedding\n",
    "            current_word_emb = captions_emb[:, t, :]  # [batch, emb_size]\n",
    "\n",
    "            # 2.5. feed lstm with current token embedding concatenated with context vector\n",
    "            lstm_input = torch.cat([current_word_emb, context], dim=1)  # [batch, emb_size + cnn_channels]\n",
    "\n",
    "            # 2.6. update lstm hidden and cell vectors\n",
    "            h_t, c_t = self.lstm(lstm_input, (h_t, c_t))\n",
    "\n",
    "            # 2.7. store current lstm hidden state,\n",
    "            combined_output = torch.cat([h_t, context, current_word_emb], dim=1)  # [batch, lstm_units + cnn_channels + emb_size]\n",
    "            recurrent_outputs.append(combined_output)\n",
    "\n",
    "         # Stack outputs along time dimension\n",
    "        reccurent_out = torch.stack(recurrent_outputs, dim=1)  # [batch, caption_length, lstm_units + cnn_channels + emb_size]\n",
    "        attention_map = torch.stack(attention_maps, dim=1)     # [batch, caption_length, spatial_size]\n",
    "\n",
    "        # compute logits for next token probabilities\n",
    "        # based on the stored in (2.7) values (reccurent_out)\n",
    "        logits = self.logits_mlp(reccurent_out)  # [batch, caption_length, n_tokens]\n",
    "\n",
    "        # return logits and attention maps from (2.4)\n",
    "        return logits, attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760398e",
   "metadata": {},
   "source": [
    "Understood the architecture, just provides the attention + vector embeddings to the LSTMs which are stacked. and the attention is calculated between thei lstm hidden state as queries and transformed features and cnn features directly as values. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
